{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2yabX2UOjO1"
      },
      "source": [
        "# Pose Prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPQV6dS2OiJQ",
        "outputId": "b80c41ed-d676-4371-ca8c-a8a0455a8607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google drive to use resources from it\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1L3PQnSPDHE"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import ast\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense, Dropout, InputLayer\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS6E81zePW4r"
      },
      "source": [
        "# Loading the CSV data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vPoJDD2Z0QM"
      },
      "outputs": [],
      "source": [
        "def dataset_compiler(csvPath):\n",
        "  csv = pd.read_csv(csvPath, header=None)\n",
        "  dataset = []\n",
        "  labels = []\n",
        "  # iterating through all image records ie rows\n",
        "  for index, row in csv.iterrows():\n",
        "    keypoints = []\n",
        "    # iterating through each keypoint for an image\n",
        "    for column in csv.columns:\n",
        "        record = row[column]\n",
        "        # if the record being read is the total score of the pose then just skip it\n",
        "        if record[:6] == \" total\":\n",
        "          break\n",
        "        # if the record being read is the image code then add to labels array\n",
        "        label = record[:3]\n",
        "        if label == \"HIG\" or label == \"KNE\" or label == \"LOW\":\n",
        "          match label:\n",
        "            case \"HIG\":\n",
        "              labels.append(\"High Kick\")\n",
        "            case \"KNE\":\n",
        "              labels.append(\"Knee Strike\")\n",
        "            case \"LOW\":\n",
        "              labels.append(\"Low Kick\")\n",
        "            case _:\n",
        "              print(\"how did you get here?\")\n",
        "        # transforming the coordinates from the record to a numpy array\n",
        "        else:\n",
        "          # extracts the data to be 'x': 64; 'y': 111; 'score': 0.504564950284737\n",
        "          pattern = r\"{(.*?)}\"\n",
        "          matches = re.findall(pattern, record)\n",
        "          extracted_content = matches[0] if matches else None\n",
        "          # splits data by ; to have each indicidual item\n",
        "          split_values = extracted_content.split(';')\n",
        "          # extracts the actual number\n",
        "          values = [float(val.split(':')[1]) for val in split_values]\n",
        "          # adds it to the array for saving\n",
        "          keypoints.append(values)\n",
        "    dataset.append(keypoints)\n",
        "  print(\"dataset compiled!\")\n",
        "  return dataset, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTwmFZuxPPBk",
        "outputId": "0a14d4c8-b53a-44af-dc8d-e7619330d308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset compiled!\n",
            "dataset compiled!\n",
            "dataset compiled!\n"
          ]
        }
      ],
      "source": [
        "# read from CSV\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "val_dir = data_dir + '/keypoints_valid.csv'\n",
        "test_dir = data_dir + '/keypoints_test.csv'\n",
        "train_dir = data_dir + '/keypoints_train.csv'\n",
        "\n",
        "\n",
        "valid_dataset, valid_labels = dataset_compiler(val_dir)\n",
        "test_dataset, test_labels = dataset_compiler(test_dir)\n",
        "train_dataset, train_labels = dataset_compiler(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEF_gmd9EMGB"
      },
      "source": [
        "# Transforming Data Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1o4c_E75K3o"
      },
      "outputs": [],
      "source": [
        "labels = ['High Kick', 'Knee Strike', 'Low Kick']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels to numerical format\n",
        "valid_num_labels = label_encoder.fit_transform(valid_labels)\n",
        "test_num_labels = label_encoder.fit_transform(test_labels)\n",
        "train_num_labels = label_encoder.fit_transform(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq9dG9YKHedR"
      },
      "outputs": [],
      "source": [
        "train_dataset_reshaped = np.reshape(train_dataset, (len(train_dataset), -1))\n",
        "validation_dataset_reshaped = np.reshape(valid_dataset, (len(valid_dataset), -1))\n",
        "test_dataset_reshaped = np.reshape(test_dataset, (len(test_dataset), -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1YxxER6ZSFu"
      },
      "source": [
        "# Print the mapping between original string labels and numerical labels (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_mapping():\n",
        "    print(\"Train Label Mapping:\")\n",
        "    for train_labels, train_num_labels in zip(train_labels, train_num_labels):\n",
        "        print(f\"{train_labels} -> {train_num_labels}\")\n",
        "\n",
        "    print(\"Test Label Mapping:\")\n",
        "    for test_labels, test_num_labels in zip(test_labels, test_num_labels):\n",
        "        print(f\"{test_labels} -> {test_num_labels}\")\n",
        "\n",
        "    print(\"Valid Label Mapping:\")\n",
        "    for valid_labels, valid_num_labels in zip(valid_labels, valid_num_labels):\n",
        "        print(f\"{valid_labels} -> {valid_num_labels}\")\n",
        "\n",
        "print_mapping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQuGclEPTVZ"
      },
      "source": [
        "# Building the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnTYSLuwRHuv"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Reshape((17, 3), input_shape=(17*3,)),  # Reshape input to (17 keypoints, 3 coordinates)\n",
        "\n",
        "        # Convolutional Layers\n",
        "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "        layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "        layers.Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(pool_size=2, strides=2),\n",
        "\n",
        "        # Flatten and Dense Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),  # Increasing dropout rate\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),  # Increasing dropout rate\n",
        "\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),  # Increasing dropout rate\n",
        "\n",
        "        layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDTAsh4HFDMn",
        "outputId": "b73545a6-d474-49e7-9111-679789f54755"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "opt = tf.keras.optimizers.Adam(lr=0.01)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using your training data and validate using validation data\n",
        "history = model.fit(train_dataset_reshaped, np.array(train_num_labels), epochs=50, validation_data=(validation_dataset_reshaped, np.array(valid_num_labels)), callbacks=[\n",
        "        keras.callbacks.EarlyStopping(patience=9, verbose=2, restore_best_weights=True),\n",
        "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=2)])\n",
        "\n",
        "with open(\"history.pkl\", \"wb\") as file:\n",
        "    pickle.dump(history.history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zru7oOPPYXgp",
        "outputId": "da3607fb-567a-43eb-d174-d9aa4a970c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_20 (Reshape)        (None, 17, 3)             0         \n",
            "                                                                 \n",
            " conv1d_58 (Conv1D)          (None, 17, 32)            320       \n",
            "                                                                 \n",
            " batch_normalization_116 (B  (None, 17, 32)            128       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " max_pooling1d_63 (MaxPooli  (None, 8, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_59 (Conv1D)          (None, 8, 64)             6208      \n",
            "                                                                 \n",
            " batch_normalization_117 (B  (None, 8, 64)             256       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " max_pooling1d_64 (MaxPooli  (None, 4, 64)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_60 (Conv1D)          (None, 4, 128)            24704     \n",
            "                                                                 \n",
            " batch_normalization_118 (B  (None, 4, 128)            512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " max_pooling1d_65 (MaxPooli  (None, 2, 128)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " batch_normalization_119 (B  (None, 64)                256       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_120 (B  (None, 32)                128       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51139 (199.76 KB)\n",
            "Trainable params: 50499 (197.26 KB)\n",
            "Non-trainable params: 640 (2.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEG_R1pZhmb-"
      },
      "source": [
        "# Metrics for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvxAkzXhK-z",
        "outputId": "caf56293-9b8e-48e5-cecd-8203a2115f97"
      },
      "outputs": [],
      "source": [
        "train_loss = history.history[\"loss\"]\n",
        "train_accuracy = history.history[\"accuracy\"]\n",
        "\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset_reshaped, np.array(test_num_labels))\n",
        "\n",
        "print(\"Train Loss:\", np.mean(train_loss))\n",
        "print(\"Train Accuracy:\", np.mean(train_accuracy))\n",
        "print()\n",
        "print(\"Val Loss:\", np.mean(val_loss))\n",
        "print(\"Val Accuracy:\", np.mean(val_accuracy))\n",
        "print()\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwKvbpndjutl"
      },
      "source": [
        "# Make predictions and output results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhyOk1p0jlkn"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x=test_dataset_reshaped, steps=len(test_dataset_reshaped), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82geKOSdkRNx",
        "outputId": "43d2a2fb-8a28-4f4a-8215-ca42981730a0"
      },
      "outputs": [],
      "source": [
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(classification_report(test_num_labels, predicted_labels))\n",
        "print(confusion_matrix(test_num_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOTgV3oNeYqY",
        "outputId": "e9973345-d1c5-4923-ed8f-a01f97e29978"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "model.save(\"model_5_V3.h5\") # legacy warning\n",
        "model.save(\"model_5_V3.keras\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
